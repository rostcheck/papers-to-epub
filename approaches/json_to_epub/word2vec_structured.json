{
  "metadata": {
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "authors": [
      {
        "name": "Tomas Mikolov",
        "affiliation": "Google Inc., Mountain View, CA",
        "email": "tmikolov@google.com"
      },
      {
        "name": "Kai Chen", 
        "affiliation": "Google Inc., Mountain View, CA",
        "email": "kaichen@google.com"
      },
      {
        "name": "Greg Corrado",
        "affiliation": "Google Inc., Mountain View, CA", 
        "email": "gcorrado@google.com"
      },
      {
        "name": "Jeffrey Dean",
        "affiliation": "Google Inc., Mountain View, CA",
        "email": "jeff@google.com"
      }
    ],
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
    "keywords": ["word representations", "neural networks", "vector space", "word2vec", "CBOW", "skip-gram"],
    "publication_info": {
      "venue": "arXiv preprint",
      "date": "2013-01-16",
      "arxiv_id": "1301.3781v3",
      "document_class": "workshop paper"
    }
  },
  
  "sections": [
    {
      "id": "introduction",
      "title": "1. Introduction",
      "level": 1,
      "content": "Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words). However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques. With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words. For example, neural network based language models significantly outperform N-gram models.",
      "subsections": [
        {
          "id": "goals",
          "title": "1.1 Goals of the Paper",
          "level": 2,
          "content": "The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100. We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(\"King\") - vector(\"Man\") + vector(\"Woman\") results in a vector that is closest to the vector representation of the word Queen. In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities, and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data."
        },
        {
          "id": "previous_work",
          "title": "1.2 Previous Work", 
          "level": 2,
          "content": "Representation of words as continuous vectors has a long history. A very popular model architecture for estimating neural network language model (NNLM) was proposed in Bengio et al., where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others. Another interesting architecture of NNLM was presented in previous work, where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora, and some of the resulting word vectors were made available for future research and comparison. However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in previous work, with the exception of certain version of log-bilinear model where diagonal weight matrices are used."
        }
      ]
    },
    {
      "id": "model_architectures",
      "title": "2. Model Architectures",
      "level": 1,
      "content": "Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words; LDA moreover becomes computationally very expensive on large data sets. Similar to previous work, to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity. For all the following models, the training complexity is proportional to O = E × T × Q, where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture. Common choice is E = 3 - 50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation.",
      "subsections": [
        {
          "id": "nnlm",
          "title": "2.1 Feedforward Neural Net Language Model (NNLM)",
          "level": 2,
          "content": "The probabilistic feedforward neural network language model has been proposed in Bengio et al. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation. The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10, the size of the projection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V. Thus, the computational complexity per each training example is Q = N × D + N × D × H + H × V, where the dominating term is H × V."
        },
        {
          "id": "rnnlm", 
          "title": "2.2 Recurrent Neural Net Language Model (RNNLM)",
          "level": 2,
          "content": "Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks. The RNN model does not have a projection layer; only input, hidden and output layer."
        }
      ]
    },
    {
      "id": "new_models",
      "title": "3. New Log-linear Models", 
      "level": 1,
      "content": "In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.",
      "subsections": [
        {
          "id": "cbow",
          "title": "3.1 Continuous Bag-of-Words Model",
          "level": 2,
          "content": "The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word. Training complexity is then Q = N × D + D × log₂(V)."
        },
        {
          "id": "skip_gram",
          "title": "3.2 Continuous Skip-gram Model", 
          "level": 2,
          "content": "The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples."
        }
      ]
    },
    {
      "id": "results",
      "title": "4. Results",
      "level": 1,
      "content": "To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big - biggest and small - smallest. We further denote two pairs of words with the same relationship as a question, as we can ask: \"What is the word that is similar to small in the same sense as biggest is similar to big?\" Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute: vector(\"biggest\") - vector(\"big\") + vector(\"small\"). This method is further referred to as the 3CosAdd.",
      "subsections": [
        {
          "id": "task_description",
          "title": "4.1 Task Description",
          "level": 2,
          "content": "To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, so that the vocabulary is not too diverse. Furthermore, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions."
        }
      ]
    },
    {
      "id": "examples",
      "title": "5. Examples of the Learned Relationships",
      "level": 1,
      "content": "Table 5 shows words that follow various relationships. We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that assumes exact match, the results in Table 5 would score only about 60%). We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications."
    },
    {
      "id": "conclusion",
      "title": "6. Conclusion", 
      "level": 1,
      "content": "In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. An interesting task where the word vectors have been shown to significantly outperform the previous state of the art is the SemEval-2012 task 2. The publicly available RNN vectors do not perform well on this task, and the NNLM vectors do not either. However, the new vectors from the CBOW and Skip-gram models provide the best performance 48.0% and 48.3% accuracy respectively, far above the previous best result of 39.2%."
    }
  ],

  "tables": [
    {
      "id": "table1",
      "caption": "Examples of five types of semantic and nine types of syntactic questions in the Semantic-Syntactic Word Relationship test set.",
      "position": "after_section_task_description",
      "headers": ["Type of relationship", "Word Pair 1", "Word Pair 2"],
      "rows": [
        ["Common capital city", "Athens Greece", "Oslo Norway"],
        ["All capital cities", "Astana Kazakhstan", "Harare Zimbabwe"], 
        ["Currency", "Algeria dinar", "Angola kwanza"],
        ["City-in-state", "Chicago Illinois", "Stockton California"],
        ["Man-Woman", "brother sister", "grandson granddaughter"],
        ["Adjective to adverb", "amazing amazingly", "apparent apparently"],
        ["Opposite", "acceptable unacceptable", "aware unaware"],
        ["Comparative", "bad worse", "big bigger"],
        ["Superlative", "bad worst", "big biggest"],
        ["Present Participle", "code coding", "dance dancing"],
        ["Nationality adjective", "Switzerland Swiss", "Cambodia Cambodian"],
        ["Past tense", "dancing danced", "decreasing decreased"],
        ["Plural nouns", "mouse mice", "dollar dollars"],
        ["Plural verbs", "work works", "speak speaks"]
      ],
      "styling": "semantic-syntactic-table"
    },
    {
      "id": "table2", 
      "caption": "Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used.",
      "position": "after_section_results",
      "headers": ["Dimensionality", "50M words", "100M words", "200M words", "400M words"],
      "rows": [
        ["50", "13.4%", "16.1%", "17.6%", "17.2%"],
        ["100", "17.8%", "24.2%", "26.4%", "24.3%"],
        ["300", "23.2%", "27.2%", "30.7%", "33.3%"],
        ["600", "24.3%", "27.5%", "33.4%", "39.5%"]
      ],
      "styling": "results-table"
    },
    {
      "id": "table3",
      "caption": "Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set.",
      "position": "after_section_results", 
      "headers": ["Architecture", "Semantic Accuracy", "Syntactic Accuracy", "Total Questions"],
      "rows": [
        ["RNNLM", "9%", "36%", "MSR"],
        ["NNLM", "23%", "53%", "All"],
        ["CBOW", "24%", "64%", "All"],
        ["Skip-gram", "55%", "59%", "All"]
      ],
      "styling": "comparison-table"
    },
    {
      "id": "table4",
      "caption": "Examples of the word pair relationships, using the best word vectors from Table 2 (Skip-gram model trained on 783M words with 300 dimensionality).",
      "position": "after_section_examples",
      "headers": ["Relationship", "Example 1", "Example 2", "Example 3"],
      "rows": [
        ["France - Paris + Rome", "Italy", "Spain", "Germany"],
        ["big - bigger + small", "smaller", "tiny", "little"],
        ["Miami - Florida + Texas", "Dallas", "Houston", "Austin"],
        ["Einstein - scientist + Picasso", "painter", "artist", "musician"],
        ["Sarkozy - France + Italy", "Berlusconi", "Monti", "Prodi"],
        ["copper - Cu + gold", "Au", "Ag", "Pt"],
        ["Boston - Massachusetts + Maryland", "Baltimore", "Annapolis", "Salisbury"],
        ["Japan - sushi + Germany", "bratwurst", "schnitzel", "strudel"],
        ["man - woman + aunt", "uncle", "nephew", "cousin"],
        ["building - architect + software", "programmer", "developer", "engineer"]
      ],
      "styling": "examples-table"
    }
  ],

  "figures": [
    {
      "id": "figure1",
      "caption": "The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.",
      "position": "after_section_cbow",
      "type": "architecture_diagram",
      "description": "Diagram showing the two proposed architectures: CBOW (Continuous Bag-of-Words) and Skip-gram models with their input/output relationships",
      "alt_text": "Architecture comparison between CBOW and Skip-gram models"
    }
  ],

  "references": [
    {
      "id": "ref1",
      "authors": ["Bengio, Y.", "Ducharme, R.", "Vincent, P.", "Jauvin, C."],
      "title": "A neural probabilistic language model",
      "venue": "Journal of Machine Learning Research",
      "year": "2003",
      "pages": "1137-1155"
    },
    {
      "id": "ref2",
      "authors": ["Hinton, G. E."],
      "title": "Learning distributed representations of concepts",
      "venue": "Proceedings of the eighth annual conference of the cognitive science society",
      "year": "1986",
      "pages": "1-12"
    },
    {
      "id": "ref3",
      "authors": ["Mikolov, T.", "Karafiát, M.", "Burget, L.", "Černocký, J.", "Khudanpur, S."],
      "title": "Recurrent neural network based language model",
      "venue": "Interspeech",
      "year": "2010",
      "pages": "1045-1048"
    }
  ],

  "citations": [
    {"text": "popular N-gram model used for statistical language modeling", "ref_id": "stupid-backoff", "position": "section_1_paragraph_1"},
    {"text": "distributed representations of words", "ref_id": "ref2", "position": "section_1_paragraph_2"},
    {"text": "neural network based language models significantly outperform N-gram models", "ref_id": "ref1", "position": "section_1_paragraph_2"},
    {"text": "feedforward neural network with a linear projection layer", "ref_id": "ref1", "position": "section_1_2_paragraph_1"}
  ],

  "equations": [
    {
      "id": "eq1",
      "content": "O = E \\times T \\times Q",
      "description": "Training complexity formula where E is number of training epochs, T is number of words in training set, Q is model-specific complexity",
      "position": "section_2_paragraph_2"
    },
    {
      "id": "eq2", 
      "content": "Q = N \\times D + N \\times D \\times H + H \\times V",
      "description": "Computational complexity for NNLM where N is context size, D is projection layer size, H is hidden layer size, V is vocabulary size",
      "position": "section_2_1_paragraph_2"
    }
  ]
}
